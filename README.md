# Mini-GPT

This code defines a language model that can be trained to generate text based on a given starting prompt. The model architecture includes a TokenAndPositionEmbedding layer, TransformerBlock layers, and a custom callback TextGenerator for generating text during training.

Here's a breakdown of the code and its functionality:

Importing Libraries: The code starts by importing the necessary libraries, including TensorFlow and Keras modules for building and training the model.

Causal Attention Mask: The causal_attention_mask function creates a mask for the upper triangle of the attention matrix in the self-attention mechanism of the Transformer. This mask prevents information flow from future tokens to current tokens.

TransformerBlock Class: This class defines a single Transformer block. It includes multi-head self-attention, feed-forward neural networks, layer normalization, and dropout layers.

TokenAndPositionEmbedding Class: This class defines the embedding layer that combines token embeddings and positional embeddings for the input sequences.

Model Configuration: The code specifies various hyperparameters such as vocabulary size, sequence length, embedding dimensions, number of attention heads, feed-forward dimensions, etc. It also defines the create_model function that assembles the complete text generation model using the defined components.

Data Loading and Preprocessing: The code downloads and extracts the IMDb movie review dataset, creates a dataset from text files, and applies text vectorization to tokenize the input text data. It also defines a function to prepare inputs and labels for the language model.

TextGenerator Callback: The TextGenerator callback generates text from the trained model at the end of each epoch. It takes a starting prompt, samples the next token, and continues generating text until a specified number of tokens are generated.

Training the Model: The main part of the code trains the language model using the provided dataset. It creates an instance of the model, tokenizes the starting prompt, and fits the model to the dataset. During training, the TextGenerator callback is used to generate text at specified intervals.

Overall, this code sets up a text generation model based on the Transformer architecture and trains it on the IMDb movie review dataset. The trained model can generate text that continues from a given starting prompt. Make sure you have the required libraries and dependencies installed, and you may need to adjust paths and other configurations based on your environment.

*Few examples of text generated by this code*

Prompt: the best scene

Generated Text: 
1) the best scene is a film , the best movie , and i would have been a [UNK] in this movie , even worse than i was not seen . but i 've ever seen . it 's no [UNK] " . the movie
2) the best scene i 've ever seen in a long long time . the story is simple . i mean and what i really did . the story was very slow , the plot was very slow and the plot is just plain stupid
3) the best scene i have seen in a few times , which i have seen in the movie . the story is simple and a very entertaining , the whole movie is not only about it , but the movie is about a woman
4) the best scene is that you have seen since 1983 's the movie is a great example when the movie has a great deal of suspense . the story revolves around a man who lives with a [UNK] [UNK] ) and a [UNK] of
5) the best scene of michael [UNK] is a great example of the most touching story ever told . the story that has the great acting in this movie is so far better than that . this movie is really about as it , i
6) the best scene ever ! ! the movie is one of the most disturbing movies i will be in my opinion that this one was one of my favorite . if i had to sit back home and watch it again . i have
7) the best scene from scene in this movie is really funny .                                
8) the best scene i 've seen the first time . the movie starts out slowly but then builds builds builds builds builds builds . builds builds builds builds builds builds builds builds builds builds builds builds in tension and builds builds . builds builds
